# --- LLM Configuration ---
# LLM_PROVIDER can be 'ollama' or 'openai' (for any OpenAI-compatible API like LM Studio)
LLM_PROVIDER="ollama"

# For 'ollama', this is the full URL to the Ollama server
# For 'openai' providers, this is the base URL of the API endpoint
LLM_ENDPOINT="http://<IP_OR_HOSTNAME_OF_LLM_SERVER>:11434"
LLM_MODEL_NAME="your-model-name" # e.g., hf.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:UD-Q4_K_XL
LLM_NUM_CTX=4096

# API Key for OpenAI-compatible endpoints. For local servers (like LM Studio),
# this can often be left blank or set to a dummy value like "NA".
LLM_API_KEY=""

# --- Secondary (Small) Model Configuration (Optional) ---
# If you leave these values empty, they will automatically fall back to using the
# primary model's configuration settings above.
#LLM_ENDPOINT_SMALL=""
#LLM_MODEL_NAME_SMALL=""
#LLM_NUM_CTX_SMALL=""

STT_SERVER_URL="http://<IP_OF_FASTER_WHISPER_SERVER>:8000/v1/audio/transcriptions"
TTS_ENGINE="coqui"
TTS_SERVER_URL="http://<IP_OF_COQUI_OR_PIPER_SERVER>:<PORT>/api/tts"
DATABASE_PATH="database/long_term_memory.db"
WAKE_WORD="Chanakya"